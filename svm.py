# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b0hrPKIkH2HV-kKb3qaz5lDxLdK_8PVW
"""

# Cell 1: Importing Libraries and Loading the Dataset

# Basic Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA

# Upload file in Colab environment
# Run this cell and upload the mushroom.csv when prompted
from google.colab import files
uploaded = files.upload()

# Automatically detect uploaded file name
file_name = list(uploaded.keys())[0]

# Load the dataset
df = pd.read_csv(file_name)

# Display basic information
print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())

# Check for missing values
print("\nMissing Values per Column:")
print(df.isnull().sum())

# Display first few rows
df.head()

# Cell 2: Exploratory Data Analysis (EDA)

# Basic statistical summary (non-numeric since most are categorical)
print("Unique values per column:\n")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

# Class distribution
plt.figure(figsize=(6,4))
sns.countplot(x=df['class'], palette='Set2')
plt.title("Class Distribution (Edible vs Poisonous)")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

# Plot distribution for a few categorical features
categorical_cols = df.columns[1:6]  # sample few columns
fig, axes = plt.subplots(2, 3, figsize=(15,8))
for i, col in enumerate(categorical_cols):
    sns.countplot(x=df[col], ax=axes[i//3, i%3], palette='coolwarm')
    axes[i//3, i%3].set_title(f"Distribution of {col}")
    axes[i//3, i%3].tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.show()

# Convert categorical columns to numeric temporarily for correlation
df_encoded = df.copy()
for col in df_encoded.columns:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])

# Correlation heatmap
plt.figure(figsize=(12,10))
sns.heatmap(df_encoded.corr(), cmap='coolwarm')
plt.title("Feature Correlation Heatmap (Encoded)")
plt.show()

# Cell 3: Data Preprocessing

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Copy dataset
data = df.copy()

# Encode all categorical variables
encoder = LabelEncoder()
for col in data.columns:
    data[col] = encoder.fit_transform(data[col])

# Separate features and target
X = data.drop('class', axis=1)
y = data['class']

# Split dataset (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("‚úÖ Data Preprocessing Complete")
print(f"Training samples: {X_train.shape[0]}")
print(f"Testing samples: {X_test.shape[0]}")

# Cell 4: SVM Implementation and Model Training

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Initialize a basic SVM model (RBF kernel as default)
svm_model = SVC(kernel='rbf', random_state=42)

# Train the model
svm_model.fit(X_train, y_train)

# Predict on test data
y_pred = svm_model.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("‚úÖ SVM Model Evaluation Results:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-score  : {f1:.4f}")
print("\nDetailed Classification Report:\n")
print(classification_report(y_test, y_pred))

# Cell 5: Visualization of SVM Results (Decision Boundaries)

from sklearn.decomposition import PCA
import numpy as np

# Reduce features to 2 dimensions for visualization
pca = PCA(n_components=2)
X_train_2D = pca.fit_transform(X_train)
X_test_2D = pca.transform(X_test)

# Train SVM again on 2D data for visualization
svm_vis = SVC(kernel='rbf', random_state=42)
svm_vis.fit(X_train_2D, y_train)

# Create meshgrid
x_min, x_max = X_train_2D[:, 0].min() - 1, X_train_2D[:, 0].max() + 1
y_min, y_max = X_train_2D[:, 1].min() - 1, X_train_2D[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),
                     np.linspace(y_min, y_max, 500))

# Predict over meshgrid
Z = svm_vis.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot decision boundary
plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
plt.scatter(X_train_2D[:, 0], X_train_2D[:, 1], c=y_train, s=30, edgecolor='k', cmap='coolwarm', label="Training Data")
plt.scatter(X_test_2D[:, 0], X_test_2D[:, 1], c=y_test, s=60, edgecolor='k', cmap='coolwarm', marker='x', label="Test Data")
plt.title("SVM Decision Boundary Visualization (After PCA Reduction)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

# Cell 6: Parameter Tuning and Optimization

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)

# Fit to training data
grid_search.fit(X_train, y_train)

# Best parameters and model
best_params = grid_search.best_params_
best_svm = grid_search.best_estimator_

print("‚úÖ Best Parameters Found:")
print(best_params)

# Evaluate tuned model
y_pred_tuned = best_svm.predict(X_test)

accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
precision_tuned = precision_score(y_test, y_pred_tuned)
recall_tuned = recall_score(y_test, y_pred_tuned)
f1_tuned = f1_score(y_test, y_pred_tuned)

print("\n‚úÖ Tuned SVM Model Evaluation Results:")
print(f"Accuracy  : {accuracy_tuned:.4f}")
print(f"Precision : {precision_tuned:.4f}")
print(f"Recall    : {recall_tuned:.4f}")
print(f"F1-score  : {f1_tuned:.4f}")

# Cell 7: Comparison & Analysis

# Compare performance of multiple kernels
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
results = []

for kernel in kernels:
    model = SVC(kernel=kernel, C=best_params.get('C', 1), gamma=best_params.get('gamma', 'scale'), random_state=42)
    model.fit(X_train, y_train)
    y_pred_kernel = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred_kernel)
    prec = precision_score(y_test, y_pred_kernel)
    rec = recall_score(y_test, y_pred_kernel)
    f1 = f1_score(y_test, y_pred_kernel)

    results.append({
        'Kernel': kernel,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-score': f1
    })

# Convert to DataFrame
results_df = pd.DataFrame(results)
print("‚úÖ SVM Kernel Comparison Results:")
display(results_df)

# Visualize comparison
plt.figure(figsize=(8,5))
sns.barplot(data=results_df.melt(id_vars='Kernel', var_name='Metric', value_name='Score'),
            x='Kernel', y='Score', hue='Metric')
plt.title('SVM Performance Comparison Across Kernels')
plt.ylim(0,1)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# --- Humanized Summary ---
print("üîç Interpretation:")
print("""
1. The RBF kernel generally performs the best on the Mushroom dataset, as it handles nonlinear decision boundaries effectively.
2. Linear kernel may perform slightly lower because feature relationships in this dataset are not purely linear.
3. Polynomial and sigmoid kernels tend to underperform when data is high-dimensional or when parameter tuning is limited.
4. Overall, SVM proves robust and accurate for binary classification problems like mushroom edibility prediction.
5. In real-world use, SVM is effective for tasks such as text classification, bioinformatics, and image recognition where decision boundaries are complex.
""")

